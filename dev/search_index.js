var documenterSearchIndex = {"docs":
[{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"EditURL = \"https://github.com/ilabcode/ActionModels.jl/blob/main/docs/src/Using_the_package/fitting_an_agent_model_to_data.jl\"","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/#Fitting-an-agent-model-to-data","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"","category":"section"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"The core of behavioral and cognitive computational modelling is to fit our models to data. One of the many essential features in the ActionModels.jl package is to fit your data with high speed and good stable performance.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"In this section the following will be demonstrated","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"Recap of fitting models\nThe fit_model() function\ntutorial of fitting one and more parameters\nPlotting posteriors","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/#So-far-with-the-Actionmodels.jl-package","page":"Fitting an agent model to data","title":"So far with the Actionmodels.jl package","text":"","category":"section"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"During the tutorial you should be comfortable with the terms agents, actions, action models, states and parameters as well as how to simulate actions. We can deifne a premade agent or create a custom agent with different kinds of action models. You can change the parameters of the agents and simulate actions by giving them inputs. You might have tried out for yourself how simulated actions change depending on which parameter values you set in your agent.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"This can lead os to what is meant by \"fitting\". We will again reference the illustration of comparing simulation and fitting:","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"(Image: Image1)","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"When we fit, we know the actions and inputs. As we have seen earlier with different parameter settings for agents, these change their \"behavior\" and actions quite drastically . When we fit the parameters of a model, we try to find the parameter values which make that model most likely to produce observed actions. Finding good guesses to these parameter values can be usefull when examining differences between groups in experimental settings.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"When we fit one or more parameters we need to set priors to sample from. These priors are initial guesses to where the plausable parameter values could be. These priors can with great benefit be informed priors, where values from previous studies influence your choice of prior.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/#The-fit_model()-function","page":"Fitting an agent model to data","title":"The fit_model() function","text":"","category":"section"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"The fit_ model() function takes the following inputs:","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"(Image: Image1)","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"Let us run through the inputs to the function one by one.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"agent::Agent: a specified agent created with either premade agent or init_agent.\nparampriors::Dict: priors (written as distributions) for the parameters you wish to fit. e.g. priors = Dict(\"learning\\rate\" => Uniform(0, 1))\ninputs:Array: array of inputs.\nactions::Array: array of actions.\nfixed_parameters::Dict = Dict(): fixed parameters if you wish to change the parameter settings of the parameters you dont fit\nsampler = NUTS(): specify the type of sampler. See Turing documentation for more details on sampler types.\nn_iterations = 1000: iterations pr. chain.\nn_chains = 1: amount of chains.\nverbose = true: set to false to hide warnings\nshow_sample_rejections = false: if set to true, get a message every time a sample is rejected.\nimpute_missing_actions = false : if true, include missing actions in the fitting process.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/#Tutorial-of-basic-use-of-fit_model()","page":"Fitting an agent model to data","title":"Tutorial of basic use of fit_model()","text":"","category":"section"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"In the first part of the tutorial we will not change the sampling and parallellization configurations or the error information / misssing actions settings. We will only work with the agent information and priors configurations. See elaborated use of fit_model() for working with more configurations Let us use a premade agent:","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"using ActionModels\nusing Distributions\nusing StatsPlots\n\nagent = premade_agent(\"premade_binary_rw_softmax\")","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"Let's give the agent some input and simulate a set of actions:","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"inputs = [1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1]\n\nactions = give_inputs!(agent, inputs)","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"Let's take a look at what parameter in the agent we could fit:","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"get_parameters(agent)","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"We want to fit learning_rate. Let us define a prior for the parameter. We set this prior as a normal distribution with mean 1 and standard deviation 0.5. It is important to note that it is possible to fit multiple parameters at the same time. In that case you would simply add more priors in the dict.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"priors = Dict(\"learning_rate\" => Normal(1, 0.5))","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"When we set a prior for a parameter it overwrites the parameter in the agent during the fitting process. The prior is not part of the agent, but is only used in the fit_model() function.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"We have our agent, inputs, actions and priors. This is what we need to fit.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"fitted_model = fit_model(agent, priors, inputs, actions)","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"As output you are presented with the summary statistics.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"We can plot the chains and distribution of the two chains.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"plot(fitted_model)","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/#Plotting-functions","page":"Fitting an agent model to data","title":"Plotting functions","text":"","category":"section"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"For plotting the prior against the posterior use the plot_parameter_distribution function.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"The first argument in the fuction is the fitted model and the second are the priors. The plot is a vizuialisation comparing the fitted parameters compared to priors","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"plot_parameter_distribution(fitted_model, priors)","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"You can extract the posterior parameters from a Turing chain with the get_posteriors() function:","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"get_posteriors(fitted_model)","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/#Fitting-multiple-parameters","page":"Fitting an agent model to data","title":"Fitting multiple parameters","text":"","category":"section"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"By adding multiple parameter priors you can autimatically fit them with fit_model. Let's say you also want to fit softmax_action_precision.","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"Add an extra prior in the Dict","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"multiple_priors =\n    Dict(\"learning_rate\" => Normal(1, 0.5), \"softmax_action_precision\" => Normal(0.8, 0.2))\n\nmultiple_fit = fit_model(agent, multiple_priors, inputs, actions)","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"Plot the parameter distribution","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"plot_parameter_distribution(multiple_fit, multiple_priors)","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"Extract the posteriors from the Turing chain","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"get_posteriors(multiple_fit)","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"","category":"page"},{"location":"markdowns/fitting_an_agent_model_to_data/","page":"Fitting an agent model to data","title":"Fitting an agent model to data","text":"This page was generated using Literate.jl.","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"EditURL = \"https://github.com/ilabcode/ActionModels.jl/blob/main/docs/src/Using_the_package/variations_of_util.jl\"","category":"page"},{"location":"markdowns/variations_of_util/#Variations-of-utility-functions","page":"Variations of utility functions","title":"Variations of utility functions","text":"","category":"section"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"This section contains a list of convenient utility functions. Many of these are used throughout the documentation and you can find all here.","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Content","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"getting states of an agent with: get_states()\ngetting parameters of an agent with: get_parameters()\nsetting parameters of an agent with: set_parameters()\ngetting history of an agent with: get_history()\nresetting an agent with reset!() and getting posteriors with get_posteriors()","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"We will define an agent to use during demonstrations of the utility functions:","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"using ActionModels #hide\n\nagent = premade_agent(\"premade_binary_rw_softmax\")","category":"page"},{"location":"markdowns/variations_of_util/#Getting-States","page":"Variations of utility functions","title":"Getting States","text":"","category":"section"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"The get_states() function can give you a single state, multiple states and all states of an agent.","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Let's start with all states","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_states(agent)","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Get a single state","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_states(agent, \"value\")","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Get multiple states","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_states(agent, [\"value\", \"action\"])","category":"page"},{"location":"markdowns/variations_of_util/#Getting-Parameters","page":"Variations of utility functions","title":"Getting Parameters","text":"","category":"section"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_parameters() work just like get_states, but will give you the parameters of the agent:","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"lets start with all parameters","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_parameters(agent)","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Get a single parameter","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_parameters(agent, (\"initial\", \"value\"))","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Get multiple parameters","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_parameters(agent, [(\"initial\", \"value\"), \"learning_rate\"])","category":"page"},{"location":"markdowns/variations_of_util/#Setting-Parameters","page":"Variations of utility functions","title":"Setting Parameters","text":"","category":"section"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Setting a single parameter in an agent","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"set_parameters!(agent, (\"initial\", \"value\"), 1)","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Setting multiple parameters in an agent","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"set_parameters!(agent, Dict(\"learning_rate\" => 3, \"softmax_action_precision\" => 0.5))","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"See the parameters we have set uising get_parameters function","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_parameters(agent)","category":"page"},{"location":"markdowns/variations_of_util/#Getting-History","page":"Variations of utility functions","title":"Getting History","text":"","category":"section"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"To get the history we need to give inputs to the agent. Let's start by giving a single input","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"give_inputs!(agent, 1)","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"We can now get the history of the agent's states. We can have a look at the \"value\" state.","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_history(agent, \"value\")","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Get multiple states' histories","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_history(agent, [\"value\", \"action\"])","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Lastly, get all history of the agent","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_history(agent)","category":"page"},{"location":"markdowns/variations_of_util/#Getting-Posteriors","page":"Variations of utility functions","title":"Getting Posteriors","text":"","category":"section"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_posteirors() is a funcion for extracting parameters from a Turing chain. Let us set up a fitted model:","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Let us reset our agent and make it ready for new input","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"reset!(agent)","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Define a range of inputs","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"inputs = [1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1]","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Define actions","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"actions = give_inputs!(agent, inputs)","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Set a prior for the parameter we wish to fit","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"using Distributions\npriors =\n    Dict(\"softmax_action_precision\" => Normal(1, 0.5), \"learning_rate\" => Normal(1, 0.1))","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"Fit the model","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"fitted_model = fit_model(agent, priors, inputs, actions)","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"We can now use the get_posteriors()","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"get_posteriors(fitted_model)","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"","category":"page"},{"location":"markdowns/variations_of_util/","page":"Variations of utility functions","title":"Variations of utility functions","text":"This page was generated using Literate.jl.","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"EditURL = \"https://github.com/ilabcode/ActionModels.jl/blob/main/docs/src/Using_the_package/premade_agents_and_models.jl\"","category":"page"},{"location":"markdowns/premade_agents_and_models/#Creating-your-agent-using-premade-agents-and-models-in-the-ActionModels-package","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"","category":"section"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"In this section we will demonstrate:","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"Defining an agent with the premade_agent() function\nGet an overview of the parameters and states\nSet one and more parameter values in an agent\nDefining an agent with custom parameter values","category":"page"},{"location":"markdowns/premade_agents_and_models/#Defining-an-agent-with-premade_agent()","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Defining an agent with premade_agent()","text":"","category":"section"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"using ActionModels #hide","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"you can define a premade agent using the premade_agent() function. The function call is the following:","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"#premade_agent(model_name::String, config::Dict = Dict(); verbose::Bool = true)","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"Model_name is the type of premade agent you wish to use. You can get a list of premade agents with the command:","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"premade_agent(\"help\")","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"Lets create an agent. We will use the \"premade_binary_rw_softmax\" agent with the \"binary_rw_softmax\" action model. You define a default premade agent with the syntax below:","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"agent = premade_agent(\"premade_binary_rw_softmax\")","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"In the Actionmodels.jl package, an agent struct consists of the action model name (which can be premade or custom), parameters and states. The premade agents are initialized with a set of configurations for parameters, states and initial state parameters.","category":"page"},{"location":"markdowns/premade_agents_and_models/#Get-an-overview-of-the-parameters-and-states-in-a-premade-agent","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Get an overview of the parameters and states in a premade agent","text":"","category":"section"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"Let us have a look at the parameters in our predefined agent with the fucntion get_parameters","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"get_parameters(agent)","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"If you wish to get one of the parameters, you can change the method of the function by specifying the parameter you wish to retrieve:","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"get_parameters(agent, \"learning_rate\")","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"Now, let us have a look at the states of the agent with the get_states() function.","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"get_states(agent)","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"the \"value\" state is initialized with an initial state parameter (this is the (\"initial\", \"value\") parameter seen in the get_parameters(agent) call). The other states are missing due to the fact, that the agent has not recieved any inputs. You can get specific states by inputting the state you wish to retrieve. This will get more interesting once we give the agents inputs.","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"get_states(agent, \"value\")","category":"page"},{"location":"markdowns/premade_agents_and_models/#Set-one-and-more-parameter-values-in-an-agent","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Set one and more parameter values in an agent","text":"","category":"section"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"As mentioned above, the premade agent is equipped with dedault parameter values. If you wish to change these values you can do it in different ways. We can change one of the parameters in our agent like below by specifying parameter and the value to set the parameter to.","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"set_parameters!(agent, \"learning_rate\", 1)\n\nget_parameters(agent)","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"If you wish to change multiple parameters in the agent, you can define a dict of parameters folowed by the value like this","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"set_parameters!(\n    agent,\n    Dict(\n        \"learning_rate\" => 0.79,\n        \"softmax_action_precision\" => 0.60,\n        (\"initial\", \"value\") => 1,\n    ),\n)","category":"page"},{"location":"markdowns/premade_agents_and_models/#Defining-an-agent-with-custom-parameter-values","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Defining an agent with custom parameter values","text":"","category":"section"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"If you know which parameter values you wish to use when defining your agent, you can specify them in the beginning as a dict() with parameter name as a string followed by the value.","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"agent_custom_parameters = premade_agent(\n    \"premade_binary_rw_softmax\",\n    Dict(\n        \"learning_rate\" => 0.7,\n        \"softmax_action_precision\" => 0.8,\n        (\"initial\", \"value\") => 1,\n    ),\n)\n\n#and we can retrieve the new parameters with the get_parameters() function\n\nget_parameters(agent_custom_parameters)","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"","category":"page"},{"location":"markdowns/premade_agents_and_models/","page":"Creating your agent using premade agents and models in the ActionModels package","title":"Creating your agent using premade agents and models in the ActionModels package","text":"This page was generated using Literate.jl.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"EditURL = \"https://github.com/ilabcode/ActionModels.jl/blob/main/docs/src/Using_the_package/creating_own_action_model.jl\"","category":"page"},{"location":"markdowns/creating_own_action_model/#Building-your-own-action-model","page":"Building your own action model","title":"Building your own action model","text":"","category":"section"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"In this section we will demonstrate the following","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"A structure comparison between an agent and an action model\nTutorial on how to make the Rescorla-Wagner softmax action model\nCreating a custom agent to the action model with init_agent()","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"An action model should always return an action distribution from which an aciton can be sampled.","category":"page"},{"location":"markdowns/creating_own_action_model/#visual-structure-of-agent-and-action-model","page":"Building your own action model","title":"visual structure of agent and action model","text":"","category":"section"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"(Image: Image1)","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"let us start by the left side of the model. We have an agent with a set of defined parameters, states and a setting.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"When initializing an action model you can start by reading in the agent's parameters. It can depend on the action model how many states you use during the update step. In this example we only use one state (which is the one that has an initial state parameter).","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"During the update step we compute the remaining two states in the agent. State_2 and state_3 are outputs of the update step which are saved in the agent's history. In our example structure you can see that the updated_state_3 is the parameter in our action distribution.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"When building a custom action model you will have to (roughly) think of 3 things:","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"What type of action do I want to simulate?\nWhat does the update step constist of?\nWhat variables (parameters and states) are used in the update step and which do I wish to save in the agent history?","category":"page"},{"location":"markdowns/creating_own_action_model/#Building-the-Rescorla-Wagner-action-model","page":"Building your own action model","title":"Building the Rescorla-Wagner action model","text":"","category":"section"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"In this example we will construct an action model according to Rescorla-Wagner. First, let us recall the parameters and states needed for this action model.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"We need two parameters: learning rate and softmax action precision. These parameters are specified when defining the agent. The main state for the update step is the \"input value\" state, and this is the only value we need to have in order to calculate an action.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"It can be nice to save other elements from the action model as states for analysis and plotting. We would in this case like to save the action probability and the transformed value in probability space. These two states don't influence the model in new trials, but are just extra output from the action model.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"Let us build our action model. The structure of the action model is equal to the strucutre in the figure.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"First, let us define the function name and which input it takes:","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"using ActionModels\nusing Distributions","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"In the next section you will be introduced to premade agents and models.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"Rescorla Wagner continuous","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"function continuous_rescorla_wagner_softmax(agent, input)\n\n    # Read in parameters from the agent\n    learning_rate = agent.parameters[\"learning_rate\"]\n\n    # Read in states with an initial value\n    old_value = agent.states[\"value\"]\n\n    ##We dont have any settings in this model. If we had, we would read them in as well.\n    ##-----This is where the update step starts -------\n\n    ##Get new value state\n    new_value = old_value + learning_rate * (input - old_value)\n\n\n    ##-----This is where the update step ends -------\n    ##Create Bernoulli normal distribution our action probability which we calculated in the update step\n    action_distribution = Distributions.Normal(new_value, 0.3)\n\n    ##Update the states and save them to agent's history\n\n    agent.states[\"value\"] = new_value\n    agent.states[\"input\"] = input\n\n    push!(agent.history[\"value\"], new_value)\n    push!(agent.history[\"input\"], input)\n\n    # return the action distribution to sample actions from\n    return action_distribution\nend\n\n#Define agent\n\nparameters = Dict(\"learning_rate\" => 1, (\"initial\", \"value\") => 0)","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"We set the initial state parameter for \"value\" state because we need a starting value in the update step.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"Let us define the states in the agent:","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"states = Dict(\"value\" => missing, \"input\" => missing)\n\n\nagent =\n    init_agent(continuous_rescorla_wagner_softmax, parameters = parameters, states = states)\n\n\ninputs = [1, 2, 3, 4, 6, 4, 10, 2, 1]\n\ngive_inputs!(agent, inputs)\n\nplot_trajectory(agent, \"action\")\nplot_trajectory!(agent, \"input\")\n\nreset!(agent)","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"With binary inputs","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"inputs = [0, 1, 0, 0, 1, 1, 1, 0, 0, 1]\ngive_inputs!(agent, inputs)","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"plot_trajectory(agent, \"action\")\nplot_trajectory!(agent, \"input\")","category":"page"},{"location":"markdowns/creating_own_action_model/#A-Binary-Rescorla-Wagner","page":"Building your own action model","title":"A Binary Rescorla-Wagner","text":"","category":"section"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"function custom_rescorla_wagner_softmax(agent, input)\n\n    # Read in parameters from the agent\n    learning_rate = agent.parameters[\"learning_rate\"]\n    action_precision = agent.parameters[\"softmax_action_precision\"]\n\n    # Read in states with an initial value\n    old_value = agent.states[\"value\"]\n\n    ##We dont have any settings in this model. If we had, we would read them in as well.\n    ##-----This is where the update step starts -------\n\n    # Sigmoid transform the value\n    old_value_probability = 1 / (1 + exp(-old_value))\n\n    ##Get new value state\n    new_value = old_value + learning_rate * (input - old_value_probability)\n\n    ##Pass through softmax to get action probability\n    action_probability = 1 / (1 + exp(-action_precision * new_value))\n\n    ##-----This is where the update step ends -------\n    ##Create Bernoulli normal distribution our action probability which we calculated in the update step\n    action_distribution = Distributions.Bernoulli(action_probability)\n\n    ##Update the states and save them to agent's history\n\n    agent.states[\"value\"] = new_value\n    agent.states[\"transformed_value\"] = 1 / (1 + exp(-new_value))\n    agent.states[\"action_probability\"] = action_probability\n\n    push!(agent.history[\"value\"], new_value)\n    push!(agent.history[\"transformed_value\"], 1 / (1 + exp(-new_value)))\n    push!(agent.history[\"action_probability\"], action_probability)\n\n    # return the action distribution to sample actions from\n    return action_distribution\nend","category":"page"},{"location":"markdowns/creating_own_action_model/#Create-An-Agent","page":"Building your own action model","title":"Create An Agent","text":"","category":"section"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"We can define the agent now. Let us do it with the init_agent() function. We need to define parameters, states and action model.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"#Set the parameters:\n\nparameters =\n    Dict(\"learning_rate\" => 1, \"softmax_action_precision\" => 1, (\"initial\", \"value\") => 0)","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"We set the initial state parameter for \"value\" state because we need a starting value in the update step.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"Let us define the states in the agent:","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"states = Dict(\n    \"value\" => missing,\n    \"transformed_value\" => missing,\n    \"action_probability\" => missing,\n)","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"And lastly the action model:","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"action_model = custom_rescorla_wagner_softmax","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"We can now initialize our agent with the action model, parameters and states.","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"agent = init_agent(action_model, parameters = parameters, states = states)\n\ninputs = [1, 0, 0, 0, 1, 1, 0, 1, 0]\n\ngive_inputs!(agent, inputs)\n\nplot_trajectory(agent, \"action\")","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"","category":"page"},{"location":"markdowns/creating_own_action_model/","page":"Building your own action model","title":"Building your own action model","text":"This page was generated using Literate.jl.","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"EditURL = \"https://github.com/ilabcode/ActionModels.jl/blob/main/docs/src/Using_the_package/prior_predictive_sim.jl\"","category":"page"},{"location":"markdowns/prior_predictive_sim/#Predictive-Simulations","page":"Predictive Simulations","title":"Predictive Simulations","text":"","category":"section"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"In order to set a good prior, a prior predicitve simulation is a principled method for testing","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"If you wish to fit a certain parameter in your agent, you can use the prior predictive simulation to simulate different states e.g. actions with parameter values sampled from this prior.","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"Overview","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"What prior/posterior predictive simulation is\nIntroduction to the plot_predictive_simulation() function\nexample of prior predictive simulation\nexample of posterior predictive simulation","category":"page"},{"location":"markdowns/prior_predictive_sim/#Predictive-simulations","page":"Predictive Simulations","title":"Predictive simulations","text":"","category":"section"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"When we do prior predictive simulations, it is often with the goal of setting a reasonable prior based on our expectations on what range our target state might be in. An example could be, that we want to fit a target state influencing actions measured in reaction time. Our prior for this parameter should not make the agent produce negative reaction times during the simulation. Let us take a look at the figure illustrating the process:","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"(Image: Image1)","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"We have a prior of one (or more) target parameters which we want to fit, and take a random sample from the prior distribution. This prior is placed in the agent alongside with earlier specified fixed parameters. We simulate forward and give the agent inputs. Depending on which target state you are interested in, you get the history of that state.","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"When the agent has been given all inputs it is reset to default, a new sample from the prior is drawn and the process is repeated the amount of times you have set the n_samples to.","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"It is possible to insert a fitted model as your posterior as well. With the exact same method (we simply sample from the posterior instead of prior) you can see how well the fitted parameters perform in retrieving plausable target states.","category":"page"},{"location":"markdowns/prior_predictive_sim/#The-plot_predictive_simulation()","page":"Predictive Simulations","title":"The plot_predictive_simulation()","text":"","category":"section"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"The inputs to the function are the following","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"(Image: Image1)","category":"page"},{"location":"markdowns/prior_predictive_sim/#Example-of-prior-predictive-simulaiton","page":"Predictive Simulations","title":"Example of prior predictive simulaiton","text":"","category":"section"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"Let us go through a prior predictive simulation","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"#load packages\nusing ActionModels\n\n#Define an agent\nagent = premade_agent(\"premade_binary_rw_softmax\")\n#Define input\ninputs = [1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0]\n\n#see which taget state wish to plot\nget_states(agent)\n#we choose action probability\ntarget_state = \"action_probability\"\n\n#see which parameter we wish to simulate from\nget_parameters(agent)\n\n#Let us choose leanring rate, and set a prior\nusing Distributions\nprior_learning_rate = Dict(\"learning_rate\" => Normal(1.2, 0.5))","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"Insert values in the function","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"using Plots\nusing StatsPlots\nplot_predictive_simulation(prior_learning_rate, agent, inputs, target_state)","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"The red dot in the plot shows the median simulated value for action probability from each run on the inputs with different parameter samples.","category":"page"},{"location":"markdowns/prior_predictive_sim/#Posterior-predictive-simulations","page":"Predictive Simulations","title":"Posterior predictive simulations","text":"","category":"section"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"To produce a posterior predictive simulation plot we need a posterior from a fitted model. To see in depth how to fit an agent model to data using fit_model(), see section [INSERT LINK] We will fit the learning rate using the prior we set earlier.  The only thing we need to fit, is actions. Let's simulate actions with give_inputs()","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"actions = give_inputs!(agent, inputs)\n\nfitted_model = fit_model(agent, prior_learning_rate, inputs, actions)","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"The plot_predictive_simulation() function recognizes a fitted model, more specifically a turing chain of posteriors. We can therefore input the fitted model as our posterior.","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"Insert the fitted model (with a posterior for learnig rate) and plot action probability as our target state.","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"plot_predictive_simulation(fitted_model, agent, inputs, target_state)","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"","category":"page"},{"location":"markdowns/prior_predictive_sim/","page":"Predictive Simulations","title":"Predictive Simulations","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/#Agents,-states-and-action-models","page":"Agents, states and action models","title":"Agents, states and action models","text":"","category":"section"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"As a very general structue we can illustrate the relation between input and action/response as below. ","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"(Image: Image1) We can generate actions based on inputs. How we use the input to generate actions happens according to the action model","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"An action model is a way of assuming how an agents actions are generated from inputs. An action model can be modulated according to a specific experimental use or how we assume a specific agent operates. ","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"The mapping between inputs and actions can be extended. We will add the following elements which all action models operate with: parameters and states. The new expanded version is seen below.","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"(Image: Image2) We can extend the action model arrow with parameters and states. Parameters are stable and contribute as constants to the system. States change and evolve according to input and parameters (the way states change happens accordingly to the structure of the action model).","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"When defining an agent, you also have to define an action model for the agent to use. you also define the states, parameters, and an optional substruct (see advanced usage of the agent).","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"We will introduce a very standard reinforcement learning action model, the binary Rescorla-Wagner softmax. The parameters in this model are learning rate and action precision. The states of the agent, who produces actions according to the Rescorla-Wagner softmax action model, are \"value\", \"transformed value\" and \"action probability\". ","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"––SOMETHING MORE ON RW ––- ?","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"When initializing an agent, we may in some cases need a starting value for certain states. These initial values are categorized as an 'initial state parameter'. In the premade Rescorla-Wagner agent the \"value\" state is initialized at 0 by default. The learning rate parameter and the action presicion are both set to 1. ","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"The transformed value is calculated based on the input value (in the first run the initial state parameter for the \"value\" state) as seen in the equation below.  ","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"$","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"\\hat{q}{n-1} =\\frac{1}{1+exp(-q{n-1})} $","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"From this we can compute the new value from which an action probability can be calculated.","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"$","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"qn = \\hat{q}{n-1}+ \\alpha \\cdot (un-\\hat{q}{n-1})$","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"$","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"p=  \\frac{1}{1+exp(-\\beta \\cdot q_n)} $","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"The last state \"action probability\" is the mean of an Bernoulli distribution from which an action can be sampled.","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"An agent is defined with a set of states and parameters. The action model defines through equations with  how the states change according to inputs, updates the history of an agent (its states), and returns an action distribution. ","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"The process described above is what we define as simulating actions. We know the inputs, parameters and states of the agent and returns actions. We can reverse this process and infer the parameters of an agent based on their actions in response to inputs. ","category":"page"},{"location":"Conceptual_introduction/agent_and_actionmodel/","page":"Agents, states and action models","title":"Agents, states and action models","text":"This will be further elaborated in fitting vs. simulating","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"EditURL = \"https://github.com/ilabcode/ActionModels.jl/blob/main/docs/src/Using_the_package/Introduction.jl\"","category":"page"},{"location":"markdowns/Introduction/#Welcome-to-the-ActionModels.jl-package!","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"","category":"section"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"ActionModels.jl is a powerfull and novel package for computational modelling of behavior and cognition. The package is developed with a intention to make computaitonal modelling intuitive, fast and easily adaptive to your experimental and simulation needs.","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"With ActionModels.jl you can define a fully customizable behavioral model and easily fit them to experimental data and used for computational modelling.","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"we provide a consice introduction to this framework for computational modelling of behvior and cognition and its accompanying terminology.","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"After this introduction, you will be presented with a detailed step-by-step guide on how to use ActionModels.jl to make your computational model runway-ready.","category":"page"},{"location":"markdowns/Introduction/#Getting-started","page":"Welcome to the ActionModels.jl package!","title":"Getting started","text":"","category":"section"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"Defning a premade agent","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"using ActionModels","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"Find premade agent, and define agent with default parameters","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"premade_agent(\"help\")\n\nagent = premade_agent(\"premade_binary_rw_softmax\")","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"Set inputs and give inputs to agent","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"inputs = [1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1]\nactions = give_inputs!(agent, inputs)\n\nusing StatsPlots\nplot_trajectory(agent, \"action_probability\")","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"Fit learning rate. Start by setting prior","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"using Distributions\npriors = Dict(\"learning_rate\" => Normal(0.5, 0.5))","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"Run model","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"chains = fit_model(agent, priors, inputs, actions, n_chains = 1, n_iterations = 10)","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"Plot prior and posterior","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"plot_parameter_distribution(chains, priors)","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"Get posteriors from chains","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"get_posteriors(chains)","category":"page"},{"location":"markdowns/Introduction/#Computaitonal-modelling-of-behavior","page":"Welcome to the ActionModels.jl package!","title":"Computaitonal modelling of behavior","text":"","category":"section"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"In computational behavioral modelling the aim is to produce mechanistic models of the processes underlying observed behavior. Importantly, to validate and test models, or to use them to distinguish groups of individuals, models must be fitted to emprircal data, an often complicated process that this package is designed to make as simple as possible.","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"You can imagine an experimental setting where a participant is to press one of two buttons. One of the buttons elict a reward and the other elict a small electrical chock. The experimenter has set up a system, so that the probability of choosing the right button shifts over time. You can imagine this as being 80/20 on left being reward-button in the first 10 trials, and then the probability shifts to 20/80 on left being rewarding in the last 10 trials.","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"The participant wishes to press the reward button and avoid the electrical chock. During the first 10 trials the participant would, through feedback, alter their behavior to select the left button the most due to the reward distribution between buttons. In the last 10 trials the preffered button press would swich.","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"After the experiement the hypothetical data could be the participants button choice. We can model the behvaior of the participant in the experiment with an agent and an action model. We can with the action model approximate how information is processed in the participant (in modelling the participant becomes the agent), and how the agent produces actions with a specific model. We try to create an artificially mapping between input and action that can explain a persons behavior","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"The agent contains a set of \"states\" and \"parameters\". The parameters in an agent are analogous to some preconcieved belief of the agent that can't be changed during the experiment. A very caricatured example could be if the participant had a strong color preference for one of the buttons which influences their decisions on a constant level. When we model behvaior and set these parameters they are related to some theoretically grounded elements from the action model. We will later build an action model from scratch where real parameters will show up.","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"The states in an agent change over time, and the way they change depend on the action model. This structure will be elaborated more on in the next section where we go into depth with agents and action models.","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"","category":"page"},{"location":"markdowns/Introduction/","page":"Welcome to the ActionModels.jl package!","title":"Welcome to the ActionModels.jl package!","text":"This page was generated using Literate.jl.","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"EditURL = \"https://github.com/ilabcode/ActionModels.jl/blob/main/docs/src/Using_the_package/Simulation_with_an_agent.jl\"","category":"page"},{"location":"markdowns/Simulation_with_an_agent/#Simulating-actions-with-an-agent","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"","category":"section"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"We will in this section introduce action simulation with the Actionmodels.jl package.","category":"page"},{"location":"markdowns/Simulation_with_an_agent/#contents-of-this-section","page":"Simulating actions with an agent","title":"contents of this section","text":"","category":"section"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"The give_inputs() function\nGiving a single input to the agent and retrieving history\nResetting the agent to initial state values\nGive_inputs() with a vector of inputs\nPlot State Trajectories","category":"page"},{"location":"markdowns/Simulation_with_an_agent/#Giving-Inputs-To-Agent","page":"Simulating actions with an agent","title":"Giving Inputs To Agent","text":"","category":"section"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"With the ActionModels package you can, once you have defined your agent, use the function give_inputs to simulate actions.","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"#give_inputs(agent::Agent, inputs::Real)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"When you give inputs to the agent, it produces actions according to the action model it is defined with.","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"As can be seen in the figure below, when we know all parameter values, states and the inputs we can simulate actions","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"(Image: Image1)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"The type of inputs you can give to your agent depends on the agent and the action it generates depends on the corresponding action model.","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"Let us define our agent and use the dedault parameter configurations","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"using ActionModels\n\nagent = premade_agent(\"premade_binary_rw_softmax\")","category":"page"},{"location":"markdowns/Simulation_with_an_agent/#Give-a-single-input","page":"Simulating actions with an agent","title":"Give a single input","text":"","category":"section"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"we can now give the agent a single input with the give_inputs!() function. The inputs for the Rescorla-Wagner agent are binary, so we input the value 1.","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"give_inputs!(agent, 1)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"The agent returns either \"false\" or \"true\" which translates to an action of either 0 or 1. Given that we have evolved the agent with one input the states of the agent are updated. Let's see how we recover the history and the states of the agent after one run on input. We can do this with the gethistory() function. With gethistory we can get one or more target states or get the history of all states.","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"Let us have a look at the history from all states:","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"get_history(agent)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"You can see in the \"value\" state contains two numbers. The first number is the initial state parameter which is set in the agent's configurations (see \"Creating your agent\" for more on the parameters and states). The second value in the \"value\" state is updated by the input. The three other states are initialized with \"missing\" and evolve as we give it inputs. The states in the agent are updated according to which computations the action model does with the input. For more information on the Rescorla-Wagner action model, check out the [LINK TO CHAPTER]","category":"page"},{"location":"markdowns/Simulation_with_an_agent/#Reset-Agent","page":"Simulating actions with an agent","title":"Reset Agent","text":"","category":"section"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"We would like to reset the agent to its default values with the reset!() function:","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"reset!(agent)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"As you can see below, we have cleared the history of the agent.","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"get_history(agent)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/#Multiple-Inputs","page":"Simulating actions with an agent","title":"Multiple Inputs","text":"","category":"section"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"We will now define a sequence of inputs to the agent.","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"inputs = [1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0]","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"Let's give the inputs to the agent, and see which type of actions it will generate based on the inputs","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"actions = give_inputs!(agent, inputs)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"We can in the same manner get the history of the agent's states. We will have a look at the action state:","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"get_history(agent, \"action_probability\")","category":"page"},{"location":"markdowns/Simulation_with_an_agent/#Plotting-Trajectories-of-states","page":"Simulating actions with an agent","title":"Plotting Trajectories of states","text":"","category":"section"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"we can visualize the different types of states using the function:","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"#plot_trajectory(agent::Agent, target_state::Union{String,Tuple}; kwargs...)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"The default title when using plot_trajectory() is \"state trajectory\". This can be changed by adding a title-call as below. We can plot the actions and the action probability of the agent in two seperate plots:","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"using Plots\nusing StatsPlots\n\nplot_trajectory(agent, \"action\", title = \"actions\")","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"We can change the state to plotting the action_probability","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"plot_trajectory(agent, \"action_probability\", title = \"acton probability\")","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"We can add a new linetype for the plot:","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"plot_trajectory(agent, \"action_probability\", title = \"acton probability\", linetype = :path)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"We can layer the plots by adding \"!\" at the function call. We can add the actions plot to prior action probability plot:","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"plot_trajectory!(agent, \"action\", title = \"action probability and action\")","category":"page"},{"location":"markdowns/Simulation_with_an_agent/#If-your-agent-computes-more-actions","page":"Simulating actions with an agent","title":"If your agent computes more actions","text":"","category":"section"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"If you wish to set up an agent who produces multiple actions, e.g. reaction time and a choice, you can use the \"multiple_actions()\" function. When setting up this type of agent, you define the different action models you want to use for each one of the wanted actions. Currently in the ActionModels.jl package we have not yet predefined actionmodels for different actions. For multiple actions you should define your own action models (see the advanced usage for how to do this)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"we define our two action models. A continuous and binary Rescorla Wagner:","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"using ActionModels\nusing Distributions","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"Binary Rescorla Wagner","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"function custom_rescorla_wagner_softmax(agent, input)\n\n    # Read in parameters from the agent\n    learning_rate = agent.parameters[\"learning_rate\"]\n    action_precision = agent.parameters[\"softmax_action_precision\"]\n\n    # Read in states with an initial value\n    old_value = agent.states[\"value_binary\"]\n\n    ##We dont have any settings in this model. If we had, we would read them in as well.\n    ##-----This is where the update step starts -------\n\n    # Sigmoid transform the value\n    old_value_probability = 1 / (1 + exp(-old_value))\n\n    ##Get new value state\n    new_value = old_value + learning_rate * (input - old_value_probability)\n\n    ##Pass through softmax to get action probability\n    action_probability = 1 / (1 + exp(-action_precision * new_value))\n\n    ##-----This is where the update step ends -------\n    ##Create Bernoulli normal distribution our action probability which we calculated in the update step\n    action_distributions = Distributions.Bernoulli(action_probability)\n\n    ##Update the states and save them to agent's history\n\n    agent.states[\"value_binary\"] = new_value\n    agent.states[\"transformed_value\"] = 1 / (1 + exp(-new_value))\n    agent.states[\"action_probability\"] = action_probability\n\n    push!(agent.history[\"value_binary\"], new_value)\n    push!(agent.history[\"transformed_value\"], 1 / (1 + exp(-new_value)))\n    push!(agent.history[\"action_probability\"], action_probability)\n\n    # return the action distribution to sample actions from\n    return action_distributions\nend","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"Continuous Rescorla Wagner","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"function continuous_rescorla_wagner_softmax(agent, input)\n\n    # Read in parameters from the agent\n    learning_rate = agent.parameters[\"learning_rate\"]\n\n    # Read in states with an initial value\n    old_value = agent.states[\"value_cont\"]\n\n    ##We dont have any settings in this model. If we had, we would read them in as well.\n    ##-----This is where the update step starts -------\n\n    ##Get new value state\n    new_value = old_value + learning_rate * (input - old_value)\n\n    ##-----This is where the update step ends -------\n    ##Create Bernoulli normal distribution our action probability which we calculated in the update step\n    action_distributions = Distributions.Normal(new_value, 0.3)\n\n    ##Update the states and save them to agent's history\n\n    agent.states[\"value_cont\"] = new_value\n    agent.states[\"input\"] = input\n\n    push!(agent.history[\"value_cont\"], new_value)\n    push!(agent.history[\"input\"], input)\n\n    # return the action distribution to sample actions from\n    return action_distributions\nend","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"Define an agent","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"parameters = Dict(\n    \"learning_rate\" => 1,\n    \"softmax_action_precision\" => 1,\n    (\"initial\", \"value_cont\") => 0,\n    (\"initial\", \"value_binary\") => 0,\n)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"We set the initial state parameter for \"value\" state because we need a starting value in the update step.","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"Let us define the states in the agent:","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"states = Dict(\n    \"value_cont\" => missing,\n    \"value_binary\" => missing,\n    \"input\" => missing,\n    \"transformed_value\" => missing,\n    \"action_probability\" => missing,\n)\n\n\nagent = init_agent(\n    [continuous_rescorla_wagner_softmax, custom_rescorla_wagner_softmax],\n    parameters = parameters,\n    states = states,\n)\n\n\n\ninputs = [1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n\n\n#multiple_actions(agent, inputs)","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"","category":"page"},{"location":"markdowns/Simulation_with_an_agent/","page":"Simulating actions with an agent","title":"Simulating actions with an agent","text":"This page was generated using Literate.jl.","category":"page"},{"location":"markdowns/complicated_custom_agents/","page":"Complicated custom agents: using substructs and sub modules","title":"Complicated custom agents: using substructs and sub modules","text":"EditURL = \"https://github.com/ilabcode/ActionModels.jl/blob/main/docs/src/Using_the_package/complicated_custom_agents.jl\"","category":"page"},{"location":"markdowns/complicated_custom_agents/#Complicated-custom-agents:-using-substructs-and-sub-modules","page":"Complicated custom agents: using substructs and sub modules","title":"Complicated custom agents: using substructs and sub modules","text":"","category":"section"},{"location":"markdowns/complicated_custom_agents/","page":"Complicated custom agents: using substructs and sub modules","title":"Complicated custom agents: using substructs and sub modules","text":"As an addition to an agent, you can implement a so-called substruct.","category":"page"},{"location":"markdowns/complicated_custom_agents/","page":"Complicated custom agents: using substructs and sub modules","title":"Complicated custom agents: using substructs and sub modules","text":"A subtruct is a method to extend the computational power of an agent. Instead of all update steps are present in the action model, we can add a level of updates through the substruct.","category":"page"},{"location":"markdowns/complicated_custom_agents/","page":"Complicated custom agents: using substructs and sub modules","title":"Complicated custom agents: using substructs and sub modules","text":"In a subtruct you can work with more states, define more parameters and in general fit more complex phenomena.","category":"page"},{"location":"markdowns/complicated_custom_agents/","page":"Complicated custom agents: using substructs and sub modules","title":"Complicated custom agents: using substructs and sub modules","text":"","category":"page"},{"location":"markdowns/complicated_custom_agents/","page":"Complicated custom agents: using substructs and sub modules","title":"Complicated custom agents: using substructs and sub modules","text":"This page was generated using Literate.jl.","category":"page"},{"location":"markdowns/custom_fit_model/","page":"Creating a custom fit_model() function","title":"Creating a custom fit_model() function","text":"EditURL = \"https://github.com/ilabcode/ActionModels.jl/blob/main/docs/src/Using_the_package/custom_fit_model.jl\"","category":"page"},{"location":"markdowns/custom_fit_model/#Creating-a-custom-fit_model()-function","page":"Creating a custom fit_model() function","title":"Creating a custom fit_model() function","text":"","category":"section"},{"location":"markdowns/custom_fit_model/","page":"Creating a custom fit_model() function","title":"Creating a custom fit_model() function","text":"If you wish to alter the fit_model() function to you own specific use case we have provided a compact version of the function with the basic structure for you to work on.","category":"page"},{"location":"markdowns/custom_fit_model/#Strucure-of-create_agent_model-is-the-following:","page":"Creating a custom fit_model() function","title":"Strucure of create_agent_model is the following:","text":"","category":"section"},{"location":"markdowns/custom_fit_model/#we-start-by-specifying-function-name-and-the-inputs","page":"Creating a custom fit_model() function","title":"we start by specifying function name and the inputs","text":"","category":"section"},{"location":"markdowns/custom_fit_model/","page":"Creating a custom fit_model() function","title":"Creating a custom fit_model() function","text":"@model function createagentmodel(     agent,     parampriors,     inputs,     actions,     imputemissing_actions, )","category":"page"},{"location":"markdowns/custom_fit_model/","page":"Creating a custom fit_model() function","title":"Creating a custom fit_model() function","text":"##Initialize dictionary for storing sampled parameters\nfitted_parameters = Dict()\n\n##--------- Sample parameters from the priors set, and set the parameters in the agent ---------\n\n##Give Turing prior distributions for each fitted parameter\nfor (param_key, param_prior) in param_priors\n    fitted_parameters[param_key] ~ param_prior\nend\n\n##Set agent parameters to the sampled values\nset_parameters!(agent, fitted_parameters)\nreset!(agent)\n\n## ----------- Specify settings for whether inputs are as vectors or arrays ---------\n\n##If the input is a single vector\nif inputs isa Vector\n    ##Prepare to through one value at a time\n    iterator = enumerate(inputs)\nelse\n    ##For an array, go through each row\n    iterator = enumerate(eachrow(inputs))\nend\n\n## ---------- Go through inputs and get the action probability distribution from the agent ------\n\n##For each timestep and input\nfor (timestep, input) in iterator\n    ##If no errors occur\n    try\n\n        ##Get the action probability distribution from the action model\n        action_probability_distribution = agent.action_model(agent, input)\n\n        ## ---- if one single action is made at each timestep (if only one action model is specified in the configurations ) ----\n\n        if actions isa Vector\n\n            ##If the action isn't missing, or if missing actions are to be imputed\n            if !ismissing(actions[timestep]) || impute_missing_actions\n                #Pass it to Turing\n                actions[timestep] ~ action_probability_distribution\n            end\n\n            ## ---- if multiple actions are made at each timestep (if more action models are specified in the configurations ) ----\n\n        elseif actions isa Array\n\n            ##Go throgh each action distribution\n            for (action_indx, distribution) in\n                enumerate(action_probability_distribution)\n\n                ##If the action isn't missing, or if missing actions are to be imputed\n                if !ismissing(actions[timestep, action_indx]) || impute_missing_actions\n                    ##Pass it to Turing\n                    actions[timestep, action_indx] ~ distribution\n                end\n            end\n        end\n\n        ##--------- If an error occurs -----------\n\n    catch e\n        ##If the custom errortype RejectParameters occurs\n        if e isa RejectParameters\n            ##Make Turing reject the sample\n            Turing.@addlogprob!(-Inf)\n        else\n            ##Otherwise, just throw the error\n            throw(e)\n        end\n    end\nend","category":"page"},{"location":"markdowns/custom_fit_model/","page":"Creating a custom fit_model() function","title":"Creating a custom fit_model() function","text":"end","category":"page"},{"location":"markdowns/custom_fit_model/###-Strucure-of-fit_model()","page":"Creating a custom fit_model() function","title":"## Strucure of fit_model()","text":"","category":"section"},{"location":"markdowns/custom_fit_model/#The-different-elements-in-the-code-are-seperated.","page":"Creating a custom fit_model() function","title":"The different elements in the code are seperated.","text":"","category":"section"},{"location":"markdowns/custom_fit_model/#start-by-inititializing-function-name-an-inputs","page":"Creating a custom fit_model() function","title":"start by inititializing function name an inputs","text":"","category":"section"},{"location":"markdowns/custom_fit_model/","page":"Creating a custom fit_model() function","title":"Creating a custom fit_model() function","text":"function fitmodel(     agent::Agent,     parampriors::Dict,     inputs::Array,     actions::Array;     fixedparameters::Dict = Dict(),     sampler = NUTS(),     ncores::Integer = 1,     niterations::Integer = 1000,     nchains = 2,     verbose = true,     showsamplerejections = false,     imputemissingactions::Bool = false, )     ##Store old parameters for resetting the agent later     oldparameters = getparameters(agent)","category":"page"},{"location":"markdowns/custom_fit_model/","page":"Creating a custom fit_model() function","title":"Creating a custom fit_model() function","text":"## -------------- CHECKS START ---------------- #\n\n##Unless warnings are hidden\nif verbose\n    ##If there are any of the agent's parameters which have not been set in the fixed or sampled parameters\n    if any(\n        key -> !(key in keys(param_priors)) && !(key in keys(fixed_parameters)),\n        keys(old_parameters),\n    )\n        @warn \"the agent has parameters which are not specified in the fixed or sampled parameters. The agent's current parameter values are used as fixed parameters\"\n    end\n\n    ##If a parameter has been specified both in the fixed and sampled parameters\n    if any(key -> key in keys(fixed_parameters), keys(param_priors))\n        @warn \"one or more parameters have been specified both in the fixed and sampled parameters. The fixed parameter value is used\"\n\n        ##Remove the parameter from the sampled parameters\n        for key in keys(fixed_parameters)\n            if key in keys(param_priors)\n                delete!(param_priors, key)\n            end\n        end\n    end\nend\n\n##If there are no parameters to sample\nif length(param_priors) == 0\n    ##Throw an error\n    throw(\n        ArgumentError(\n            \"no parameters to sample. Either an empty dictionary of parameter priors was passed, or all parameters with priors were also specified as fixed parameters\",\n        ),\n    )\nend\n\n##If there are different amounts of inputs and actions\nif size(inputs, 1) != size(actions, 1)\n    throw(\n        ArgumentError(\n            \"there are different amounts of inputs and actions (they differ in their first dimension). This is not supported\",\n        ),\n    )\nend\n\n### Set fixed parameters to agent ###\nset_parameters!(agent, fixed_parameters)\n\n### Run forward once ###\n##Initialize dictionary for populating with median parameter values\nsampled_parameters = Dict()\n##Go through each of the agent's parameters\nfor (param_key, param_prior) in param_priors\n    ##Add the median value to the tuple\n    sampled_parameters[param_key] = median(param_prior)\nend\n##Set sampled parameters\nset_parameters!(agent, sampled_parameters)\n##Reset the agent\nreset!(agent)\n\ntry\n    ##Run it forwards\n    test_actions = give_inputs!(agent, inputs)\n\n    ##If the model returns a different amount of actions from what was inputted\n    if size(test_actions) != size(actions)\n        throw(\n            ArgumentError(\n                \"the passed actions is a different shape from what the model returns\",\n            ),\n        )\n    end\n\ncatch e\n    ##If a RejectParameters error occurs\n    if e isa RejectParameters\n        ##Warn the user that prior median parameter values gives a sample rejection\n        if verbose\n            @warn \"simulating with median parameter values from the prior results in a rejected sample.\"\n        end\n    else\n        ##Otherwise throw the actual error\n        throw(e)\n    end\nend\n\n##--------------- FIT MODEL START ----------- #\n\n##------- Fit model with one core specified ------ #\n\n##If only one core is specified, use sequentiel sampling\nif n_cores == 1\n\n    ##Initialize Turing model\n    model =\n        create_agent_model(agent, param_priors, actions, inputs, impute_missing_actions)\n\n    ##If sample rejection warnings are to be shown\n    if show_sample_rejections\n        ##Fit model to inputs and actions, as many separate chains as specified\n        chains = map(i -> sample(model, sampler, n_iterations), 1:n_chains)\n\n        ##If sample rejection warnings are not to be shown\n    else\n        ##Create a logger which ignores messages below error level\n        sampling_logger = Logging.SimpleLogger(Logging.Error)\n        ##Use that logger\n        chains = Logging.with_logger(sampling_logger) do\n\n            ##Fit model to inputs and actions, as many separate chains as specified\n            map(i -> sample(model, sampler, n_iterations), 1:n_chains)\n        end\n    end\n\n    ##------- Fit model with one multiple cores specified ------ #\n\nelseif n_cores > 1\n\n    ##If the user has already created processsses\n    if length(procs()) == 1\n        ##Add worker processes\n        addprocs(n_cores, exeflags = \"--project\")\n\n        ##Set flag to remove the workers later\n        remove_workers_at_end = true\n    else\n        ##Error\n        @warn \"\"\"\n        n_cores was set to > 1, but workers have already been created. No new workers were created, and the existing ones are used for parallelization.\n        Note that the following variable names are broadcast to the workers: sampler agent param_priors inputs actions impute_missing_actions\n        \"\"\"\n        #Set flag to not remove the workers later\n        remove_workers_at_end = false\n    end\n\n    ##------ Setup distribution of information to processes for parallellization -----\n\n    ##Load packages on worker processes\n    @everywhere @eval using ActionModels\n    @everywhere @eval using Turing\n\n    ##Broadcast necessary information to workers\n    @everywhere sampler = $sampler\n    @everywhere agent = $agent\n    @everywhere param_priors = $param_priors\n    @everywhere inputs = $inputs\n    @everywhere actions = $actions\n    @everywhere impute_missing_actions = $impute_missing_actions\n\n    ##----- fit model to inputs with shown sample rejection warnings ------\n\n    if show_sample_rejections\n        ##Fit model to inputs and actions, as many separate chains as specified\n        chains = pmap(\n            i -> sample(\n                create_agent_model(\n                    agent,\n                    param_priors,\n                    inputs,\n                    actions,\n                    impute_missing_actions,\n                ),\n                sampler,\n                n_iterations,\n                save_state = false,\n            ),\n            1:n_chains,\n        )\n\n        ##----- fit model to inputs not showing sample rejection warnings ------\n\n    else\n        ##Create a logger which ignores messages below error level\n        sampling_logger = Logging.SimpleLogger(Logging.Error)\n        ##Use that logger\n        chains = Logging.with_logger(sampling_logger) do\n\n            ##Fit model to inputs and actions, as many separate chains as specified\n            pmap(\n                i -> sample(\n                    create_agent_model(\n                        agent,\n                        param_priors,\n                        inputs,\n                        actions,\n                        impute_missing_actions,\n                    ),\n                    sampler,\n                    n_iterations,\n                    save_state = false,\n                ),\n                1:n_chains,\n            )\n        end\n    end\n\n    ## ----- end configurations: worker cleanup and combining chains -------\n    ##If workers are to be removed\n    if remove_workers_at_end\n        ##Remove workers\n        rmprocs(workers())\n    end\n\nelse\n    throw(\n        ArgumentError(\n            \"n_cores was set to a non-positive integer. This is not supported.\",\n        ),\n    )\nend\n\n##Concatenate chains together\nchains = chainscat(chains...)\n\n##-------------- CLEANUP ----------------\n\n##Reset the agent to its original parameters\nset_parameters!(agent, old_parameters)\nreset!(agent)\n\n##Turing includes the dictionary name 'fitted_parameters' in the parameter names, so it must be removed\n##Initialize dict for replacement names\nreplacement_param_names = Dict()\n##For each parameter\nfor param_key in keys(param_priors)\n    ##Set to replace the fitted_parameters[] version with just the parameter name\n    replacement_param_names[\"fitted_parameters[$param_key]\"] = param_key\nend\n##Input the dictionary to replace the names\nchains = replacenames(chains, replacement_param_names)\n\nreturn chains","category":"page"},{"location":"markdowns/custom_fit_model/","page":"Creating a custom fit_model() function","title":"Creating a custom fit_model() function","text":"end","category":"page"},{"location":"markdowns/custom_fit_model/","page":"Creating a custom fit_model() function","title":"Creating a custom fit_model() function","text":"","category":"page"},{"location":"markdowns/custom_fit_model/","page":"Creating a custom fit_model() function","title":"Creating a custom fit_model() function","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Conceptual_introduction/fitting_vs_simulating/#Fitting-and-simulating","page":"Fitting and simulating","title":"Fitting and simulating","text":"","category":"section"},{"location":"Conceptual_introduction/fitting_vs_simulating/","page":"Fitting and simulating","title":"Fitting and simulating","text":"The actionmodels.jl package can among other features fit models and simulate actions.","category":"page"},{"location":"Conceptual_introduction/fitting_vs_simulating/","page":"Fitting and simulating","title":"Fitting and simulating","text":"As seen in the image below, the difference between fitting and simulating is essentially if we want to infer one or more parameters, or infer actions. ","category":"page"},{"location":"Conceptual_introduction/fitting_vs_simulating/","page":"Fitting and simulating","title":"Fitting and simulating","text":"(Image: Image1)","category":"page"},{"location":"Conceptual_introduction/fitting_vs_simulating/","page":"Fitting and simulating","title":"Fitting and simulating","text":"Fitting is an obvious path if you have action data from a participant, and you want to infer the parameters from which the specific actions could arise from. See using the package for how to do fit models.","category":"page"},{"location":"Conceptual_introduction/fitting_vs_simulating/","page":"Fitting and simulating","title":"Fitting and simulating","text":"Both simulating and fitting with the actionmodels.jl package is straight forward, and will be elaborated further on in the fitting a model and simulating with an agent section.","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"CurrentModule = ActionModels","category":"page"},{"location":"#ActionModels-Package","page":"ActionModels Package","title":"ActionModels Package","text":"","category":"section"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"Documentation for ActionModels. The documentation has four elements: a conceptual introduction to the package, how to use it, guides for advanced use and a list of functions.","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"","category":"page"},{"location":"#Conceptual-introduction-to-action-models","page":"ActionModels Package","title":"Conceptual introduction to action models","text":"","category":"section"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"In this part of the documentation the ideas and intuition will be grounded for using the action models module. bla bla","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"Introduction","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"Agent and actionmodel","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"Fitting and simulating","category":"page"},{"location":"#Using-the-package","page":"ActionModels Package","title":"Using the package","text":"","category":"section"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"Introduction\nAgent and actionmodel\nFitting and simulating\nCreating Your Model\nUsing Premade Agent Models\nAgent Based Simulation\nUtility Functions\nFitting An Agent Model \nPredictive Simulations\nAdvanced usage","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"Simulation with an agent","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"Creating your action model","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"fitting and agent model to data","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"prior and posterior predictive simulation","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"utility functions","category":"page"},{"location":"#Advanced-use","page":"ActionModels Package","title":"Advanced use","text":"","category":"section"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"complicated custom agents","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"Custom fit_model() function","category":"page"},{"location":"#Functions","page":"ActionModels Package","title":"Functions","text":"","category":"section"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"","category":"page"},{"location":"","page":"ActionModels Package","title":"ActionModels Package","text":"Modules = [ActionModels]","category":"page"},{"location":"#ActionModels.premade_agents","page":"ActionModels Package","title":"ActionModels.premade_agents","text":"premade_agents::Dict{String,Function}\n\nGlobal dictionary constant that contains premade agents. This is updated by other packages that add new premade agents.\n\n\n\n\n\n","category":"constant"},{"location":"#ActionModels.Agent","page":"ActionModels Package","title":"ActionModels.Agent","text":"\n\n\n\n","category":"type"},{"location":"#ActionModels.RejectParameters","page":"ActionModels Package","title":"ActionModels.RejectParameters","text":"Custom error type which will result in rejection of a sample\n\n\n\n\n\n","category":"type"},{"location":"#ActionModels.SharedParameter","page":"ActionModels Package","title":"ActionModels.SharedParameter","text":"Type for shared parameters containing both the parameter value and a vector of parameter names that will share that value\n\n\n\n\n\n","category":"type"},{"location":"#ActionModels.binary_rw_softmax-Tuple{Agent, Integer}","page":"ActionModels Package","title":"ActionModels.binary_rw_softmax","text":"binary_rw_softmax(agent::Agent, input::Bool)\n\nAction model that learns from binary inputs with a classic Rescorla-Wagner model. Passes learnt probabilities through a softmax to get the action prpbability distribution.\n\nParameters: \"learningrate\" and \"softmaxactionprecision\". States: \"value\", \"valueprobability\", \"action_probability\".\n\n\n\n\n\n","category":"method"},{"location":"#ActionModels.check_agent-Tuple{Agent}","page":"ActionModels Package","title":"ActionModels.check_agent","text":"Function for checking the structure of the agent\n\n\n\n\n\n","category":"method"},{"location":"#ActionModels.create_agent_model-NTuple{5, Any}","page":"ActionModels Package","title":"ActionModels.create_agent_model","text":"create_agent_model(agent,param_priors,inputs,actions,impute_missing_actions)\n\nCreate a Turing model object used for fitting an ActionModels agent.\n\n\n\n\n\n","category":"method"},{"location":"#ActionModels.fit_model-Tuple{Agent, Dict, Array, Array}","page":"ActionModels Package","title":"ActionModels.fit_model","text":"\"\"     fitmodel(agent::Agent, inputs::Array, actions::Vector, parampriors::Dict, kwargs...)\n\nUse Turing to fit the parameters of an agent to a set of inputs and corresponding actions.\n\nArguments\n\n'agent::Agent': an ActionModels agent object created with either premadeagent or initagent.\n'param_priors::Dict': dictionary containing priors (as Distribution objects) for fitted parameters. Keys are parameter names, values are priors.\n'inputs:Array': array of inputs. Each row is a timestep, and each column is a single input value.\n'actions::Array': array of actions. Each row is a timestep, and each column is a single action.\n'fixed_parameters::Dict = Dict()': dictionary containing parameter values for parameters that are not fitted. Keys are parameter names, values are priors. For parameters not specified here and without priors, the parameter values of the agent are used instead.\n'sampler = NUTS()': specify the type of Turing sampler.\n'n_cores = 1': set number of cores to use for parallelization. If set to 1, no parallelization is used.\n'n_iterations = 1000': set number of iterations per chain.\n'n_chains = 2': set number of amount of chains.\n'verbose = true': set to false to hide warnings.\n'showsamplerejections = false': set whether to show warnings whenever samples are rejected.\n'imputemissingactions = false': set whether the values of missing actions should also be estimated by Turing.\n\nExamples\n\n#Create a premade agent: binary Rescorla-Wagner\nagent = premade_agent(\"premade_binary_rw_softmax\")\n\n#Set priors for the learning rate\nparam_priors = Dict(\"learning_rate\" => Uniform(0, 1))\n\n#Set inputs and actions\ninputs = [1, 0, 1]\nactions = [1, 1, 0]\n\n#Fit the model\nfit_model(agent, param_priors, inputs, actions, n_chains = 1, n_iterations = 10)\n\n\n\n\n\n","category":"method"},{"location":"#ActionModels.get_history","page":"ActionModels Package","title":"ActionModels.get_history","text":"get_history(agent::Agent, target_state::Union{String,Tuple})\n\nGet the history of a single state from an agent. Returns a vector.\n\nget_history(agent::Agent, target_states::Vector)\n\nGet set of a vector of states from an agent. Returns a dictionary of states and their histories.\n\nget_history(agent::Agent)\n\nGet histories for states from an agent. Returns a dictionary of states and their histories.\n\n\n\n\n\n","category":"function"},{"location":"#ActionModels.get_parameters","page":"ActionModels Package","title":"ActionModels.get_parameters","text":"get_parameters(agent::Agent, target_param::Union{String,Tuple})\n\nGet a single parameter from an agent. Returns a single value.\n\nget_parameters(agent::Agent, target_param::Vector)\n\nGet a set of parameter values from an agent. Returns a dictionary of parameters and their values.\n\nget_parameters(agent::Agent)\n\nGet all parameters from an agent. Returns a dictionary of parameters and their values.\n\n\n\n\n\n","category":"function"},{"location":"#ActionModels.get_posteriors-Tuple{MCMCChains.Chains}","page":"ActionModels Package","title":"ActionModels.get_posteriors","text":"get_posteriors(chain::Chains; type::String = \"median\")\n\nExtract parameters from a Turing Chain object. Returns a dictionary of parameters and their posteriors.  'type' can be set to either 'median', in which case median values are extracted, or 'distribution', in which case full posterior distributions are extracted.\n\n\n\n\n\n","category":"method"},{"location":"#ActionModels.get_states","page":"ActionModels Package","title":"ActionModels.get_states","text":"get_states(agent::Agent, target_state::Union{String,Tuple})\n\nGet a single state from an agent. Returns a single value.\n\nget_states(agent::Agent, target_state::Vector)\n\nGet a set of state values from an agent. Returns a dictionary of state names and their values.\n\nget_states(agent::Agent)\n\nGet all states from an agent. Returns a dictionary of state names and their values.\n\n\n\n\n\n","category":"function"},{"location":"#ActionModels.give_inputs!","page":"ActionModels Package","title":"ActionModels.give_inputs!","text":"give_inputs!(agent::Agent, inputs)\n\nGive inputs to an agent. Input can be a single value, a vector of values, or an array of values. Returns the agent's action trajectory, without the initial state.\n\n\n\n\n\n","category":"function"},{"location":"#ActionModels.init_agent-Tuple{}","page":"ActionModels Package","title":"ActionModels.init_agent","text":"init_agent(action_model::Function; substruct::Any = nothing, parameters::Dict = Dict(), states::Union{Dict, Vector} = Dict(),\nsettings::Dict = Dict(), shared_parameters::Dict = Dict())\n\nInitialize an agent. \n\nNote that actionmodel can also be specified as a vector of action models: actionmodel::Vector{Function}. In this case the action models will be stored in the agent's settings. In that case use the function 'multiple_actions'\n\nArguments\n\n'action_model::Function': a function specifying the agent's action model. Can be any function that takes an agent and a single input as arguments, and returns a probability distribution from which actions are sampled.\n'substruct::Any = nothing': struct containing additional parameters and states. This structure also get passed to utility functions. Check advanced usage guide.\n'parameters::Dict = Dict()': dictionary containing parameters of the agent. Keys are parameter names (strings, or tuples of strings), values are parameter values.\n'states::Union{Dict, Vector} = Dict()': dictionary containing states of the agent. Keys are state names (strings, or tuples of strings), values are initial state values. Can also be a vector of state name strings.\n'settings::Dict = Dict()': dictionary containing additional settings for the agent. Keys are setting names, values are setting values.\n'shared_parameters::Dict = Dict()': dictionary containing shared parameters. Keys are the the name of the shared parameter, values are the value of the shared parameter followed by a vector of the parameters sharing that value.\n\nExamples\n\n```julia\n\nCreate agent with a binary Rescorla-Wagner action model\n\nCreate action model function\n\nfunction binaryrwsoftmax(agent::Agent, input::Union{Bool,Integer})\n\n#Read in parameters\nlearning_rate = agent.parameters[\"learning_rate\"]\naction_precision = agent.parameters[\"softmax_action_precision\"]\n\n#Read in states\nold_value = agent.states[\"value\"]\n\n#Sigmoid transform the value\nold_value_probability = 1 / (1 + exp(-old_value))\n\n#Get new value state\nnew_value = old_value + learning_rate * (input - old_value_probability)\n\n#Pass through softmax to get action probability\naction_probability = 1 / (1 + exp(-action_precision * new_value))\n\n#Create Bernoulli normal distribution with mean of the target value and a standard deviation from parameters\naction_distribution = Distributions.Bernoulli(action_probability)\n\n#Update states\nagent.states[\"value\"] = new_value\nagent.states[\"value_probability\"], 1 / (1 + exp(-new_value))\nagent.states[\"action_probability\"], action_probability\n#Add to history\npush!(agent.history[\"value\"], new_value)\npush!(agent.history[\"value_probability\"], 1 / (1 + exp(-new_value)))\npush!(agent.history[\"action_probability\"], action_probability)\n\nreturn action_distribution\n\nend\n\n#Define requried parameters parameters = Dict(     \"learningrate\" => 1,     \"softmaxaction_precision\" => 1,     (\"initial\", \"value\") => 0, )\n\n#Define required states states = Dict(     \"value\" => missing,     \"valueprobability\" => missing,     \"actionprobability\" => missing, )\n\n#Create agent agent = initagent(     binaryrw_softmax,     parameters = parameters,     states = states,     settings = settings, )\n\n\n\n\n\n","category":"method"},{"location":"#ActionModels.multiple_actions-Tuple{Agent, Any}","page":"ActionModels Package","title":"ActionModels.multiple_actions","text":"multiple_actions(agent::Agent, input::Any)\n\nAction model that combines multiple action models stored in the agent.\n\n\n\n\n\n","category":"method"},{"location":"#ActionModels.plot_parameter_distribution","page":"ActionModels Package","title":"ActionModels.plot_parameter_distribution","text":"plot_parameter_distribution(fitted_model, param_priors;\n    subplot_titles = Dict(),\n    show_distributions = true,\n    show_intervals = true,\n    prior_color = :green,\n    posterior_color = :orange,\n    prior_interval_offset = 0,\n    posterior_interval_offset = 0.01,\n    inner_interval = 0.5,\n    outer_interval = 0.8,\n    plot_width = 900,\n    plot_height = 300)\n\nPlot the prior and posterior distributions of the parameters of a fitted model.\n\nArguments\n\n'subplot_titles': A dictionary of parameter names and their corresponding plot titles.\n'show_distributions': Whether to show full distributions.\n'show_intervals': Whether to show uncertainty intervals.\n'prior_color': Color of the prior distribution.\n'posterior_color': Color of the posterior distribution.\n'priorintervaloffset': Offset of the prior interval bars.\n'posteriorintervaloffset': Offset of the posterior interval bars.\n'inner_interval': Size of the inner uncertainty interval.\n'outer_interval': Size of the outer uncertainty interval.\n'plot_width': Width of the plot.\n'plot_height': Height of the plot.\n\n\n\n\n\n","category":"function"},{"location":"#ActionModels.plot_predictive_simulation-Tuple{Union{Dict, MCMCChains.Chains}, Agent, Array, Union{String, Tuple}}","page":"ActionModels Package","title":"ActionModels.plot_predictive_simulation","text":"plotpredictivesimulation(paramdistributions::Union{Chains,Dict}, agent::Agent, inputs::Array, targetstate::Union{String,Tuple};     fixedparameters::Dict = Dict(), nsimulations::Int = 100, verbose::Bool = true, mediancolor::Union{String,Symbol} = :red, title::String = \"Sampled trajectories\",     label::Union{String,Tuple} = targetstate, alpha::Real = 0.1, linewidth::Real = 2, )\n\nSimulate distributions of states and actions for an agent, with parameters sampled from a specified distirbutions, and given a series of inputs.\n\nArguments\n\n'param_distributions::Union{Chains,Dict}': Distributions to sample parameters from. Can be a dictionary containing keys and distributions, or a Turing Chain object containing the posterior distributions after fitting.\n'agent::Agent': an ActionModels agent object created with either premadeagent or initagent.\n'inputs:Array': array of inputs. Each row is a timestep, and each column is a single input value.\n'target_state::Union{String,Tuple}': the state for which to plot the simulated distribution. If set to 'action', plot the action distribution. Note that the target state must be in the agent's history. \n'fixed_parameters::Dict = Dict()': dictionary containing parameter values for parameters that are not fitted. Keys are parameter names, values are priors. For parameters not specified here and without priors, the parameter values of the agent are used instead.\n'n_simulations::Int = 100': set number of simulations you want to run.\n'verbose = true': set to false to hide warnings.\n'median_color::Union{String,Symbol} = :red': specify color of median value in the plot.\n'title::String = \"Sampled trajectories\"': title on graph.\n'label::Union{String,Tuple} = target_state': label on graph.\n'alpha::Real = 0.1': the transparency of each simulated trajectory line.\n'linewidth::Real = 2': specify linewidth on your plot.\n\n\n\n\n\n","category":"method"},{"location":"#ActionModels.plot_trajectory","page":"ActionModels Package","title":"ActionModels.plot_trajectory","text":"plot_trajector(agent::Agent, target_state::Union{String,Tuple}; kwargs...)\n\nPlot trajectory of a state from an agent. Keyword arguments are passed to Plots.\n\n\n\n\n\n","category":"function"},{"location":"#ActionModels.premade_agent","page":"ActionModels Package","title":"ActionModels.premade_agent","text":"premade_agent(model_name::String, config::Dict = Dict(); verbose::Bool = true)\n\nCreate an agent from the list of premade agents.\n\nArguments\n\n'model_name::String': Name of the premade model. Returns a list of possible model names if set to 'help'. \n'config::Dict = Dict()': A dictionary with configurations for the agent, like parameters and settings.\n'verbose::Bool = true': If set to false, warnings are hidden.\n\n\n\n\n\n","category":"function"},{"location":"#ActionModels.reset!-Tuple{Agent}","page":"ActionModels Package","title":"ActionModels.reset!","text":"reset!(agent::Agent)\n\nReset an agent to its initial state. Use initial state parameters when specified, and otherwise just use the first value in the state history.\n\n\n\n\n\n","category":"method"},{"location":"#ActionModels.set_parameters!","page":"ActionModels Package","title":"ActionModels.set_parameters!","text":"set_parameters!(agent::Agent, target_param::Union{String,Tuple}, param_value::Any)\n\nSetting a single parameter value for an agent.\n\nset_parameters!(agent::Agent, parameter_values::Dict)\n\nSet mutliple parameters values for an agent. Takes a dictionary of parameter names and values.\n\n\n\n\n\n","category":"function"},{"location":"#ActionModels.single_input!-Tuple{Agent, Any}","page":"ActionModels Package","title":"ActionModels.single_input!","text":"single_input!(agent::Agent, input::Any)\n\nGive a single input to an Agent and let it evolve. Returns the agent's action.\n\n\n\n\n\n","category":"method"},{"location":"#ActionModels.warn_premade_defaults","page":"ActionModels Package","title":"ActionModels.warn_premade_defaults","text":"warn_premade_defaults(defaults::Dict, config::Dict, prefix::String = \"\")\n\nCheck if any config values have not been set by the user, and warns them that it is using defaults.\n\n\n\n\n\n","category":"function"}]
}
